# total params: 1987139
# model params
model_name: mamba
dataset: lra_retrieval

embed_dim: 128
N: 8
n_layers: 4
ssm_dropout: [0.0, 0.1]
dropout: [0.0, 0.1]
# other hyperparameters
data_to_use_percent: 0.05
train_batch_size: 32
validation_batch_size: 32

seq_len: 4000
initial_lr: 1e-3
init_fin_lr_ratio: 0.1

save_csv: true
save_model: true

use_A_dropout: true
use_cuda_scan: true

num_iterations: 1
num_epochs: 50
